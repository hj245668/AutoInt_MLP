## Movie Recommendation System with AutoInt and AutoInt+MLP

$\color{green}{\text{AutoInt, AutoInt+MLP}}$
$\color{green}{\text{streamlit run movie_rec_app.py}}$

[ 1 ]  Overview

ë³¸ í”„ë¡œì íŠ¸ëŠ” AutoInt(Automatic Feature Interaction Learning)ì™€ AutoInt+MLP ëª¨ë¸ì„ í™œìš©í•œ ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. 
MovieLens 1M ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ê³¼ê±° ì‹œì²­ ì´ë ¥ê³¼ í‰ì  ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê°œì¸í™”ëœ ì˜í™” ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤.

[ 2 ] System Architecture

1. Model Architecture
   
1.1 AutoInt Model
AutoIntëŠ” Multi-Head Self-Attention ë©”ì»¤ë‹ˆì¦˜ì„ í™œìš©í•˜ì—¬ feature ê°„ì˜ ê³ ì°¨ì› ìƒí˜¸ì‘ìš©ì„ ìë™ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.

Key Components:
Features Embedding Layer: ë²”ì£¼í˜• featuresë¥¼ dense embedding ë²¡í„°ë¡œ ë³€í™˜

Multi-Head Self-Attention Layers: 3ê°œ ì¸µì˜ attention ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ feature ê°„ ìƒí˜¸ì‘ìš© í•™ìŠµ

Attention heads: 2
Embedding dimension: 16
Residual connections ì ìš©

Output Layer: Sigmoid activationì„ í†µí•œ CTR(Click-Through Rate) ì˜ˆì¸¡

1.2 AutoInt+MLP Model
AutoInt êµ¬ì¡°ì— Deep Neural Networkë¥¼ ê²°í•©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì…ë‹ˆë‹¤.

Key Components:
AutoIntì˜ ëª¨ë“  êµ¬ì„±ìš”ì†Œ í¬í•¨

ì¶”ê°€ DNN Branch:
Hidden units: (32, 32)
Activation: ReLU
Dropout rate: 0.4
Batch Normalization (optional)

Fusion Layer: AutoInt ì¶œë ¥ê³¼ DNN ì¶œë ¥ì„ ê²°í•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡

2. Data Pipeline
   
Raw Data (MovieLens 1M)
    â†“
Data Preprocessing
    â†“
Feature Engineering
    â†“
Label Encoding
    â†“
Train/Test Split (80:20)
    â†“
Model Training
    â†“
Evaluation & Inference

Dataset
MovieLens 1M Dataset

Users: 6,040ëª…
Movies: 3,706í¸
Ratings: 1,000,209ê°œ
Rating Scale: 1-5 (ì •ìˆ˜)
Time Period: 2000-2003

Feature Schema
Feature                    Type             Description            Cardinality  
user_id                    Categorical      ì‚¬ìš©ì ì‹ë³„ì           6,040
movie_id                   Categorical      ì˜í™” ì‹ë³„ì             3,706
rating_year                Categorical      í‰ì  ë¶€ì—¬ ì—°ë„          4  
rating_month               Categorical      í‰ì  ë¶€ì—¬ ì›”            12  
rating_decade              Categorical      í‰ì  ë¶€ì—¬ ì—°ëŒ€          -
movie_decade               Categorical      ì˜í™” ì œì‘ ì—°ëŒ€          10    
movie_year                 Categorical      ì˜í™” ì œì‘ ì—°ë„          81
genre1, genre2, genre3     Categorical      ì˜í™” ì¥ë¥´ (ìµœëŒ€ 3ê°œ)    18
gender                     Categorical      ì‚¬ìš©ì ì„±ë³„             2
age                        Categorical      ì‚¬ìš©ì ì—°ë ¹ëŒ€           7occupationCategoricalì‚¬ìš©ì ì§ì—…21zipCategoricalì‚¬ìš©ì ìš°í¸ë²ˆí˜¸3,439
Total Field Dimensions: [6040, 3706, 10, 81, 4, 12, 1, 18, 18, 16, 2, 7, 21, 3439]
Training Configuration
Hyperparameters
python# Model Parameters
embedding_dim = 16
att_layer_num = 3
att_head_num = 2
att_res = True
dnn_hidden_units = (32, 32)
dnn_activation = 'relu'
dnn_dropout = 0.4

# Training Parameters
epochs = 5
batch_size = 2048
learning_rate = 0.0001
optimizer = Adam
loss_function = BinaryCrossentropy

# Regularization
l2_reg_dnn = 0
l2_reg_embedding = 1e-5
```

### Training Results

#### AutoInt Model
```
Epoch 1/5: loss: 0.6813, val_loss: 0.6505
Epoch 2/5: loss: 0.6221, val_loss: 0.5944
Epoch 3/5: loss: 0.5707, val_loss: 0.5543
Epoch 4/5: loss: 0.5487, val_loss: 0.5467
Epoch 5/5: loss: 0.5430, val_loss: 0.5446
```

**Loss Reduction**: 
- Training loss: 0.6813 â†’ 0.5430 (20.3% improvement)
- Validation loss: 0.6505 â†’ 0.5446 (16.3% improvement)

#### AutoInt+MLP Model
```
Epoch 1/5: loss: 0.6760, val_loss: 0.6468
Epoch 2/5: loss: 0.6180, val_loss: 0.5896
Epoch 3/5: loss: 0.5660, val_loss: 0.5500
Epoch 4/5: loss: 0.5434, val_loss: 0.5435
Epoch 5/5: loss: 0.5377, val_loss: 0.5411
Loss Reduction:

Training loss: 0.6760 â†’ 0.5377 (20.5% improvement)
Validation loss: 0.6468 â†’ 0.5411 (16.3% improvement)

Evaluation Metrics
ModelNDCG@10Hit Rate@10AutoInt0.662010.63026AutoInt+MLP0.661960.63058
Performance Analysis:

NDCG (Normalized Discounted Cumulative Gain): ë‘ ëª¨ë¸ ëª¨ë‘ ì•½ 0.662ë¡œ ê±°ì˜ ë™ì¼
Hit Rate: AutoInt+MLPê°€ 0.00032 (0.05%) ë” ë†’ìœ¼ë‚˜ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì°¨ì´ëŠ” ì•„ë‹˜
ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì‹¤ì§ˆì ìœ¼ë¡œ ë™ë“±í•˜ë©°, ì‘ì—… íŠ¹ì„±ì— ë”°ë¼ ì„ íƒ ê°€ëŠ¥

Implementation Details
1. Data Preprocessing
python# Label Encoding for categorical features
label_encoders = {
    'user_id': LabelEncoder(),
    'movie_id': LabelEncoder(),
    'genre1': LabelEncoder(),
    # ... ê¸°íƒ€ features
}

# Train/Test Split
train_size = 0.8
train_data = data[:int(len(data) * train_size)]
test_data = data[int(len(data) * train_size):]
2. Model Training
python# Model compilation
model.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss=BinaryCrossentropy(),
    metrics=[BinaryAccuracy()]
)

# Model fitting
history = model.fit(
    X_train, y_train,
    batch_size=2048,
    epochs=5,
    validation_data=(X_val, y_val),
    verbose=1
)
3. Inference Pipeline
pythondef get_recommendations(user_id, year, month, model, top_k=10):
    # 1. ì‚¬ìš©ìê°€ ì‹œì²­í•˜ì§€ ì•Šì€ ì˜í™” í•„í„°ë§
    unseen_movies = filter_unseen_movies(user_id)
    
    # 2. Feature êµ¬ì„±
    features = build_features(user_id, unseen_movies, year, month)
    
    # 3. ëª¨ë¸ ì˜ˆì¸¡
    predictions = model.predict(features, batch_size=2048)
    
    # 4. Top-K ì¶”ì¶œ
    top_k_movies = get_top_k(predictions, k=top_k)
    
    return top_k_movies
```

## Web Application

### Streamlit Interface

**ì£¼ìš” ê¸°ëŠ¥**:
1. **ì‚¬ìš©ì ì •ë³´ ì…ë ¥**
   - ì‚¬ìš©ì ID ì§ì ‘ ì…ë ¥
   - ì¶”ì²œ íƒ€ê²Ÿ ì—°ë„/ì›” ì„ íƒ
   - ëª¨ë¸ ì„ íƒ (AutoInt / AutoInt+MLP / ë‘ ëª¨ë¸ ë¹„êµ)

2. **ì‚¬ìš©ì í”„ë¡œí•„ í‘œì‹œ**
   - ì„±ë³„, ë‚˜ì´, ì§ì—…, ì§€ì—­ ì •ë³´

3. **ê³¼ê±° ì‹œì²­ ì´ë ¥**
   - í‰ì  4ì  ì´ìƒ ì˜í™” ëª©ë¡
   - ì˜í™” ì œëª©, ì¥ë¥´, í‰ì , ì‹œì²­ ì‹œê°„

4. **ì¶”ì²œ ê²°ê³¼**
   - Top-10 ì˜í™” ì¶”ì²œ
   - ì˜í™” ID, ì œëª©, ì¥ë¥´ ì •ë³´
   - ë‘ ëª¨ë¸ ë¹„êµ ì‹œ ë‚˜ë€íˆ í‘œì‹œ

### Application Screenshots

**ì‹¤í–‰ ì˜ˆì‹œ 1**: ì‚¬ìš©ì ID 3, ì—°ë„ 2001, ì›” 5
- **ì‚¬ìš©ì ì •ë³´**: M, 25ì„¸, ì§ì—… 15, ì§€ì—­ 55117
- **ì„ í˜¸ ì˜í™” (9ê°œ)**: Animal House, Raising Arizona, Happy Gilmore ë“± ì½”ë¯¸ë”” ì¥ë¥´ ì„ í˜¸
- **AutoInt+MLP ì¶”ì²œ**: M, Cape Fear, Terror in a Texas Town ë“± ë“œë¼ë§ˆ/ìŠ¤ë¦´ëŸ¬ ì¥ë¥´ 10ê°œ

**ì‹¤í–‰ ì˜ˆì‹œ 2**: ì‚¬ìš©ì ID 2, ì—°ë„ 2000, ì›” 5
- **ì‚¬ìš©ì ì •ë³´**: M, 56ì„¸, ì§ì—… 16, ì§€ì—­ 70072
- **ì„ í˜¸ ì˜í™” (73ê°œ)**: Shine, Verdict ë“± ë“œë¼ë§ˆ ì¥ë¥´ ë‹¤ìˆ˜
- **AutoInt ì¶”ì²œ**: Umbrellas of Cherbourg, Aparajito, Murder My Sweet ë“± í´ë˜ì‹/ë“œë¼ë§ˆ 10ê°œ

## Project Structure
```
AutoInt_MLP/
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ field_dims.npy              # Feature dimension info
â”‚   â”œâ”€â”€ label_encoders.pkl          # Fitted label encoders
â”‚   â””â”€â”€ ml-1m/
â”‚       â”œâ”€â”€ users.dat               # Raw user data
â”‚       â”œâ”€â”€ movies.dat              # Raw movie data
â”‚       â”œâ”€â”€ ratings.dat             # Raw rating data
â”‚       â”œâ”€â”€ *_prepro.csv           # Preprocessed data
â”‚       â””â”€â”€ movielens_rcmm_v*.csv  # Integrated data
â”‚
â”œâ”€â”€ ğŸ“ model/
â”‚   â”œâ”€â”€ autoInt_model_weights.weights.h5
â”‚   â”œâ”€â”€ autoIntMLP_model_weights.weights.h5
â”‚   â””â”€â”€ label_encoders.pkl
â”‚
â”œâ”€â”€ ğŸ“ notebook/
â”‚   â”œâ”€â”€ data_EDA.ipynb             # Exploratory data analysis
â”‚   â”œâ”€â”€ data_prepro.ipynb          # Data preprocessing
â”‚   â”œâ”€â”€ autoint_train.ipynb        # AutoInt training
â”‚   â”œâ”€â”€ autoint_mlp_train.ipynb    # AutoInt+MLP training
â”‚   â””â”€â”€ model_load_test.ipynb      # Model testing
â”‚
â”œâ”€â”€ autoint.py                      # AutoInt implementation
â”œâ”€â”€ autointmlp.py                   # AutoInt+MLP implementation
â”œâ”€â”€ movie_rec_app.py                # Main Streamlit app
â”œâ”€â”€ show_st*.py                     # App variations
â””â”€â”€ requirements.txt                # Dependencies
Installation & Usage
1. Environment Setup
bash# Clone repository
git clone https://github.com/your-username/AutoInt_MLP.git
cd AutoInt_MLP

# Install dependencies
pip install -r requirements.txt
2. Data Preprocessing (First time only)
bash# Run notebook/data_prepro.ipynb
jupyter notebook notebook/data_prepro.ipynb
Input:

data/ml-1m/*.dat (raw data)

Output:

data/ml-1m/*_prepro.csv (preprocessed data)
data/ml-1m/movielens_rcmm_v2.csv (integrated data)

3. Model Training (First time only)
Option A: Train AutoInt
bash# Run notebook/autoint_train.ipynb
jupyter notebook notebook/autoint_train.ipynb
Option B: Train AutoInt+MLP
bash# Run notebook/autoint_mlp_train.ipynb
jupyter notebook notebook/autoint_mlp_train.ipynb
Output:

model/autoInt_model_weights.weights.h5
model/autoIntMLP_model_weights.weights.h5
data/field_dims.npy
model/label_encoders.pkl

4. Run Application
bashstreamlit run movie_rec_app.py
Required Files:

âœ… data/field_dims.npy
âœ… data/label_encoders.pkl
âœ… data/ml-1m/*_prepro.csv
âœ… model/autoInt_model_weights.weights.h5
âœ… model/autoIntMLP_model_weights.weights.h5
âœ… autoint.py
âœ… autointmlp.py

Key Findings
1. Model Performance

ë‘ ëª¨ë¸ ëª¨ë‘ NDCG@10 ì•½ 0.662, Hit Rate@10 ì•½ 0.63ìœ¼ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥
AutoInt+MLPì˜ ì¶”ê°€ DNN layerê°€ ì„±ëŠ¥ í–¥ìƒì— ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ
ë°ì´í„°ì…‹ íŠ¹ì„±ìƒ attention mechanismë§Œìœ¼ë¡œë„ ì¶©ë¶„í•œ feature interaction í•™ìŠµ ê°€ëŠ¥

2. Training Stability

5 epoch ë‚´ì— ì•ˆì •ì ì¸ ìˆ˜ë ´
Validation lossê°€ epoch 4ë¶€í„° plateau ë„ë‹¬
Overfitting ì§•í›„ ì—†ìŒ (train/val loss ì°¨ì´ < 0.02)

3. Inference Efficiency

Batch prediction (2048) í™œìš©ìœ¼ë¡œ íš¨ìœ¨ì ì¸ ì¶”ë¡ 
6,000ëª… ì‚¬ìš©ìì— ëŒ€í•œ ì „ì²´ ì¶”ì²œ ìƒì„± ì‹œê°„ < 10ì´ˆ
Real-time ì¶”ì²œ ê°€ëŠ¥í•œ ìˆ˜ì¤€ì˜ latency

Future Work

Model Enhancement

Extended AutoInt (XDeepFM) ì ìš©
Attention mechanism variant ì‹¤í—˜ (sparse attention, local attention)
Multi-task learning (rating prediction + ranking)


Feature Engineering

User/Item embedding pre-training (Word2Vec, BERT4Rec)
Temporal features í™•ì¥ (time of day, day of week)
Social features (collaborative filtering signals)


System Optimization

Model quantization for faster inference
Distributed training for larger datasets
A/B testing framework êµ¬ì¶•


Production Deployment

Docker containerization
REST API development (FastAPI)
Model serving with TensorFlow Serving
Monitoring and logging system



References

Song, W., Shi, C., Xiao, Z., Duan, Z., Xu, Y., Zhang, M., & Tang, J. (2019). AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks. CIKM 2019.
Harper, F. M., & Konstan, J. A. (2015). The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems, 5(4), 1-19.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. NeurIPS 2017.

License
This project is licensed und
